# AI in Protein Science and Engineering: Embracing the Computational Revolution in Biochemistry
## The Computational Nature of Biochemistry: Integrating Data and Complexity

Traditionally viewed as an empirical science, biochemistry has dramatically changed, especially with the rapid growth of data generated in biological research. It's essential to recognise that today’s biochemists are not just experimentalists but also computational scientists working at the intersection of data analysis and biological understanding.

### From Reductionism to Systems Integration

Biochemistry has long relied on reductionist principles, which involve breaking down complex biological processes into their molecular components to understand how they work. However, during the last twenty years we have seen a shift towards systems biology, which examines the intricate networks of molecular interactions. Unlike the reductionist approach, systems biology examines how these components are part of dynamic systems. This perspective highlights that higher-level biological phenomena—such as how cells function—cannot always be understood by examining individual molecules.

An important concept here is **teleonomy**, which refers to the apparent purposefulness of biological systems without implying any intentional design. This understanding pushes us beyond simple molecular reductionism, urging us to view biological processes as complex, information-driven networks that reflect a level of interdependence and organisation.

### The Emergence of Computational Biochemistry

With the advancement of computational techniques, we see a growing debate within the scientific community regarding the role of computational biologists. Some critics have labelled them "research parasites," suggesting they rely on the data generated by experimentalists without contributing original experiments. However, this view needs to include the crucial role that data analysis plays in today's biochemistry. By leveraging vast amounts of data, researchers can test hypotheses and derive findings at an unprecedented scale, supporting and enhancing traditional experimental research.

Hallam Stevens points out that integrating computational tools into biochemistry is so profound that "bioinformatics" may eventually blend into everyday biological practice. In other words, using computational methods will become a standard part of conducting biological research, reshaping how scientists approach their work.

### Data Science and Computational Thinking in Biochemistry

Since we moved into the 21st century, it’s clear that biochemistry has become increasingly data-driven. **Computational thinking**—designing, analysing, and applying computational models—has become central to our understanding of biochemical processes. This change has significant educational implications, meaning that present biochemists must be well-versed in traditional biochemistry, data science, and computational approaches. 

This interdisciplinary focus means we are drawing on statistics, computer science, and applied mathematics to tackle complex biological questions. As Tony Hey and his colleagues suggest, successful scientific discovery now depends on integrating methods from various fields to achieve results that might be impossible with a single discipline alone. These combined efforts are essential for advancing biochemistry in our data-rich environment.

### tl;dr. Shaping the Present of Biochemistry

In summary, the shift towards computational methods in biochemistry reflects broader changes in the biological sciences where data analysis and computational modelling are becoming integral to scientific inquiry. Modern biochemists must interpret life as physical and informational processes where computational models can help us unravel complex biological phenomena. Understanding this blend of computational thinking and data-driven insights is crucial for the current generation of biochemists. We have crossed a new frontier of biochemistry—where data and complexity converge.


## What Does It Mean for a Machine to Learn?

We will explore a fascinating question: **How do machines learn compared to us, mere mortals?** 
### Machines: The Data Crunchers

Machines focus on statistical relationships. Machine learning algorithms are effective at identifying **statistical relationships** within large datasets. They can process massive amounts of data efficiently, enabling them to recognize patterns for predictions or decisions. they are good at **Pattern Detection** i.e., machines learn by finding patterns in data, mirroring how humans notice trends in experiments.

Machines require **substantial datasets** and specific **algorithms** to function effectively, as larger datasets generally lead to better outcomes, akin to how practice enhances human performance. Additionally, the choice of appropriate algorithms is crucial for effective learning; using the wrong algorithm can result in poor outcomes.

### Humans: The Cognitive Multitaskers

Human learning extends beyond data. We possess a remarkable ability to **abstract and generalize** concepts from our experiences, enabling us to understand complex scenarios like molecular interactions. This capability is complemented by our exceptional skills in **logical reasoning and problem-solving**, allowing us to troubleshoot and evaluate multiple factors during experiments. Additionally, **social and emotional learning** plays a crucial role in enhancing understanding and retention, as collaborative environments foster deeper insights and connections among individuals.

### Synergy Between Machine Learning and Human Expertise

Integrating machine learning with human expertise can drive scientific advancements by combining the complementary strengths of both. Machines excel at processing data quickly, while humans contribute intuition and creativity. This collaboration fosters innovation and discovery, leading to breakthroughs in various fields.

### Key Differences Summarized

- **Scope of Learning**: Machines learn specific tasks; humans have broader cognitive capabilities.
- **Nature of Knowledge**: Machines focus on statistical data; humans understand concepts and reasoning.
- **Role of Experience**: Machines learn passively; humans engage in active, experiential learning.

### Looking Ahead: The Future of Learning

Future advancements in AI may lead to machines that can emulate human cognitive abilities like reasoning and creativity, further contributing to scientific discovery. Both machines and humans play critical roles in science. **Embracing tools** like machine learning for data management allows for efficiency, while human interpretation and creativity drive progress. Ultimately, the **synergy** between machine capabilities and human curiosity is essential for scientific advancement.


### What is Machine Learning?

In today's data-driven world, **machine learning** has become a powerful tool across various fields, including biology and biochemistry. But what does it mean for a machine to "learn"? This piece introduces you to the fundamental principles of machine learning and how machines can learn from data to perform tasks and make predictions—knowledge that is increasingly relevant in modern biochemistry.

Machine learning can be understood as a process that enables computers to automatically acquire knowledge from data. Instead of relying on explicit instructions, machine learning algorithms analyze patterns and relationships within datasets. As the machine is exposed to more data, it enhances its ability to perform tasks and make predictions—a crucial advantage in the fast-evolving world of biochemistry.

### Key Components of Machine Learning Systems

A machine learning system comprises three vital elements:

1. **Representation**: This component determines how the classifier—responsible for making predictions—represents information in a way that the computer can understand. The choice of representation plays a critical role in defining the hypothesis space, which is the set of possible models the machine can explore.

2. **Evaluation**: This phase quantifies the performance of the classifier. Common metrics for evaluation include accuracy, precision, and recall, which help us understand how well the model performs on a given task.

3. **Optimization**: This focuses on finding the best-performing model within the hypothesis space based on the chosen evaluation metric.

### Learning to Generalize

One of the primary goals in machine learning is **generalization**—the ability of a model to perform effectively on previously unseen data. Unlike mere memorization of training data, generalization allows a model to apply learned patterns to make accurate predictions in novel situations.

#### The Challenge of Overfitting

A significant hurdle to achieving generalization is the problem of **overfitting**. This occurs when a model becomes too tailored to the training data, missing the broader patterns necessary for effective application to new data. To combat overfitting, we can:

- **Simplify the Model**: Choose a less complex model that captures essential patterns without being overly specific to the training data.
- **Cross-Validation**: Use techniques like cross-validation to evaluate how well our results may generalize to other datasets.
- **Regularization**: Incorporate regularization methods that introduce a complexity penalty during the optimization phase.

### The Importance of Domain Knowledge

Effective machine learning goes beyond just having data; **domain knowledge** is vital. Understanding the biological context helps in selecting the right features, algorithms, and evaluation metrics. This knowledge is crucial for:

- **Feature Selection**: Identifying the most relevant variables to include in your models.
- **Constraining the Hypothesis Space**: Reducing the risk of overfitting by limiting the models considered by the algorithm.
- **Interpreting Results**: Ensuring that the model's predictions are meaningful and biologically relevant.


### Types of Machine Learning Models
The choice of machine learning method depends on various factors, including the nature of the data, the desired task, and the available computational resources.

Machine learning methods can be broadly categorised into supervised and unsupervised learning.
- **Supervised learning** involves training a model on labelled data, where each data point has an associated output or target variable. For example, the training data in protein secondary structure prediction consists of protein sequences labelled with their known secondary structures (helix, sheet, or coil). Supervised learning aims to learn the relationship between the input features (e.g., amino acid sequence) and the output labels (e.g., secondary structure), enabling the model to predict new, unseen data. Supervised learning encompasses tasks like classification (predicting discrete class labels, e.g., predicting if a protein is an enzyme or not) and regression (predicting continuous values, e.g., e.g., predicting the binding affinity of a drug to a protein target).
- **Unsupervised learning**, in contrast, deals with unlabelled data where the desired output is unknown. These methods aim to discover patterns and structures within the data without explicit guidance. Everyday unsupervised learning tasks include:
	- Clustering: Grouping similar data points based on their features. For example, in gene expression analysis, clustering could identify groups of patients with similar expression profiles.
	- Dimensionality reduction: Reducing the number of features while preserving important information. This can be helpful for visualisation, noise reduction, and simplifying subsequent analysis. For example, principal component analysis (PCA) is a widely used dimensionality reduction technique.

The choice between supervised and unsupervised learning depends on the availability of labelled data and the specific goals of the analysis. **Linear Regression** predicts continuous outcomes based on input variables, while **Logistic Regression** is used for binary classification by estimating probabilities via a logistic function. **Support Vector Machines (SVMs)** excel in high-dimensional spaces, finding hyperplanes that separate classes with maximal margin, though they can be sensitive to kernel choices. **Decision Trees** utilize a tree-like structure to classify data by learning decision rules, making them interpretable and versatile for both categorical and numerical data. 

Additionally, ensemble methods like **Random Forests** enhance prediction accuracy by combining multiple decision trees, reducing overfitting. **K-Nearest Neighbors (KNN)** classifies based on the majority label of a data point's nearest neighbors but can be computationally intensive with larger datasets. **K-Means** clusters data points based on similarity, minimizing within-cluster variance. **Principal Component Analysis (PCA)** reduces data dimensionality, capturing variance in new, orthogonal features for improved visualization and feature selection. Lastly, **Naive Bayes** applies Bayes' theorem for probabilistic classification, making it effective in tasks like text classification and spam filtering. Each method has its strengths and applications, forming the foundation of machine learning techniques.

### Applications in Biochemistry

Machine learning is revolutionising various areas in biochemistry:
#### Protein Structure and Function Prediction:
- Secondary Structure Prediction: Machine learning models can predict the secondary structure of a protein (alpha helices, beta sheets, or coils) based on its amino acid sequence. These predictions can provide valuable insights into protein folding and function.
- Binding Site Prediction: Machine learning can be used to identify potential binding sites on proteins for ligands, including small molecules, peptides, and other proteins. This information is crucial for drug discovery and understanding protein interactions.
- Functional Annotation Transfer: Machine learning can transfer functional annotations from well-characterized proteins to those with limited experimental data, based on sequence or structural similarity.
#### Genomics and Transcriptomics:
- Gene Expression Analysis: Machine learning methods can analyze large-scale gene expression datasets to identify patterns associated with specific biological conditions, such as disease states or responses to environmental stimuli.
- Genome-Wide Association Studies (GWAS): Machine learning can be used to identify genetic variations associated with diseases or other traits by analyzing large datasets of genetic information.
#### Drug Discovery and Development:
- Virtual Screening: Machine learning models can predict the binding affinity of small molecules to protein targets, helping to prioritize candidates for experimental testing and accelerate drug discovery.
- Drug Repurposing: Machine learning can identify new therapeutic uses for existing drugs by analyzing their interactions with different biological targets.
#### Systems Biology and Network Analysis:
- Network Inference: Machine learning algorithms can infer biological networks, such as gene regulatory networks or protein-protein interaction networks, from high-throughput data.
- Pathway Analysis: Machine learning can identify key biological pathways involved in specific processes or diseases by analyzing changes in gene expression, protein levels, or metabolite concentrations.
#### Other Applications:
- Metagenomics: Analyzing complex microbial communities using machine learning techniques to identify novel species, predict functional roles, and understand community dynamics.
- Evolutionary Biology: Using machine learning to study evolutionary relationships, predict protein evolution, and analyze phylogenetic trees.
- Bioimaging: Applying machine learning to automate image analysis, segment cells and organelles, and identify patterns in microscopy images.
### Advantages of Machine Learning in Biology:
Machine learning has emerged as a powerful tool for **handling large datasets**, offering the capability to efficiently process and analyze vast amounts of data. This technological advancement enables researchers to discover patterns that may otherwise be overlooked in manual analyses. Furthermore, machine learning excels at **uncovering hidden relationships**, particularly complex, non-linear interactions between variables, which can provide valuable insights into the underlying mechanisms of biological processes. Perhaps most importantly, trained machine learning models offer **significant predictive power**, allowing researchers to make accurate predictions on new data. This predictive capability not only helps in generating hypotheses but also aids in prioritizing experiments, ultimately accelerating the pace of scientific discovery.

### tl;dr 

Machines excel at identifying patterns in large datasets through statistical relationships, while humans leverage abstract reasoning and emotional learning for deeper understanding and problem-solving. The combination of machine learning and human expertise drives innovation, enhancing data processing with human intuition. The future of learning relies on this synergy, leading to scientific advancements. Machine learning allows computers to learn from data and predict outcomes without explicit programming. As the field progresses, there's an emphasis on creating models that are both accurate and interpretable, building trust in predictions and yielding valuable biological insights.

As future biochemists, understanding machine learning will equip you with powerful tools to advance scientific discovery. Remember, the synergy between your biochemical expertise and machine learning techniques can lead to breakthroughs that might be impossible with a single discipline alone.





## Deep Learning in Biochemistry: Navigating Complex Biological Data
As biochemists exploring the frontiers of molecular science, we're increasingly encountering vast and complex datasets that hold the secrets to understanding biological systems. **Deep learning**, a powerful subset of machine learning within artificial intelligence, has emerged as a transformative tool for making sense of this data. Unlike traditional computational methods that require explicit programming for each task, deep learning models can automatically learn intricate patterns and make predictions directly from raw data.

At its core, deep learning enables computers to mimic some aspects of human learning by using algorithms inspired by the structure and function of the brain's neural networks. This capability is revolutionizing fields like genomics, proteomics, and drug discovery, where the sheer volume and complexity of data have outpaced traditional analytical methods.

### Deep Learning vs. Traditional Machine Learning

Traditional machine learning relies on manual feature extraction, where experts utilize their domain knowledge to select and engineer relevant features from raw data. These features are then fed into algorithms like decision trees or support vector machines to make predictions.

In contrast, deep learning automates the feature extraction process through **representation learning**. This allows the model to learn to represent data through multiple layers of abstraction, capturing complex patterns without the need for manual intervention. Deep learning models can analyze raw biological data—such as protein sequences or molecular structures—learning relevant features that predict crucial properties like protein folding, enzyme activity, or disease susceptibility. Thus, deep learning offers a more efficient and powerful approach to analyzing complex biological systems.

### Understanding Neural Networks: The Heart of Deep Learning

To appreciate how deep learning works, let's explore **artificial neural networks (ANNs)**, the backbone of deep learning models. These networks are inspired by the human brain's network of neurons and are designed to recognize patterns in data through layers of interconnected nodes.

In a neural network, the structure is akin to a layered web of interconnected units called neurons. The process begins with the **input layer**, which receives the raw data, such as amino acid sequences of proteins, gene expression profiles, or molecular imaging data. This data is then processed through **hidden layers**, where it is transformed and analyzed to extract features and patterns. Each hidden layer builds upon the previous one, capturing increasingly complex representations. The depth of the neural network corresponds to the number of these hidden layers it contains. Finally, the **output layer** produces the final prediction or classification, which might involve predicting the 3D structure of a protein or classifying a compound as a potential drug candidate.

### How Neural Networks Learn

The learning process of a neural network involves adjusting the connections (or **weights**) between neurons to minimize the difference between the predicted output and the actual target. Here's how it works:

#### Forward Propagation

Data flows forward from the input layer, through the hidden layers, to the output layer. Each neuron receives inputs, computes a weighted sum, applies an **activation function**, and passes the output to the next layer.

**Activation functions** introduce non-linearity, allowing the network to learn complex patterns. Common activation functions include the Sigmoid Function, which outputs values between 0 and 1 and is useful for probabilities. Another function is ReLU (Rectified Linear Unit), which outputs zero if the input is negative; otherwise, it outputs the input directly. This function helps in training deep networks by mitigating the vanishing gradient problem.

#### Backpropagation and Optimization

After making a prediction, the network evaluates how far off it was using a **loss function**, which quantifies the error.

**Backpropagation** is the process of determining how much each weight contributed to the overall error in a neural network. This is achieved by applying calculus, specifically the chain rule, which allows the network to compute the gradient of the loss function concerning each weight.

To refine these weights, **optimization algorithms** like Stochastic Gradient Descent (SGD) or Adam utilize these gradients to make adjustments aimed at minimizing the loss. This iterative process occurs over many cycles, known as epochs, until the model's predictions show significant improvement.


### How neural network learns for non experts

Understanding how neural networks learn can seem daunting at first, but let's break it down into simple terms. Think of it as a process similar to how you learn a new skill in the lab—you try, see how well you did, and adjust your approach accordingly.

#### The Basics: Adjusting Connections to Minimize Error

At the heart of a neural network's learning process is the adjustment of **weights**—these are the strengths of the connections between the neurons (the network's basic units). The goal is to fine-tune these weights so that the network's predictions get as close as possible to the actual targets.

Imagine you're adjusting the settings on a lab instrument to get the most accurate results. Similarly, the neural network adjusts its weights to minimize the difference between its predictions and the real answers.

#### Forward Propagation: Passing Data Through the Network

##### Step 1: Input Layer Receives Data
The process starts with the **input layer**, where the network receives raw data. For example, this could be a protein's amino acid sequence.

##### Step 2: Data Moves Through Hidden Layers
The data is then passed forward through one or more **hidden layers**. Each neuron in these layers performs calculations on the data it receives.

##### Step 3: Neurons Compute Weighted Sums
Each neuron takes the inputs, multiplies them by their respective **weights**, and adds them up to get a **weighted sum**. Think of it like calculating a weighted average of your lab results.

##### Step 4: Activation Functions Transform the Sum
The weighted sum is passed through an **activation function**, which introduces non-linearity. This means the network can model complex relationships, not just straight lines.

##### Step 5: Output Layer Produces Prediction
The transformed data reaches the **output layer**, which produces the network's prediction. For example, predicting whether a protein will fold into a certain structure.

#### Activation Functions Explained

**Activation functions** decide whether a neuron should be activated (i.e., whether its information should be passed forward). They add flexibility to the model.

- **Sigmoid Function**: Outputs values between 0 and 1, which are useful for probabilities. For example, if you’re predicting the likelihood of a reaction occurring, you want a value represented as a percentage between 0% and 100%.

- **ReLU (Rectified Linear Unit)**: Outputs values ranging from 0 to 1, useful for probabilities. For example, when predicting the likelihood of a reaction occurring, you want a value expressed as a percentage between 0% and 100%.

#### Backpropagation and Optimization: Learning from Mistakes

##### Step 1: Calculating the Error with a Loss Function

After making a prediction, the network needs to assess its performance. It uses a **loss function** to calculate the difference (or error) between its prediction and the actual target. This process is similar to measuring how much your experiment's results deviate from the expected values.

##### Step 2: Backpropagation—Tracing Back the Error
**Backpropagation** is the method the network uses to figure out which weights contributed most to the error. It works backward from the output layer to the input layer. Think of it as tracing back through your experimental steps to find out where an error occurred.

Backpropagation functions by calculating the **gradient** of the loss function in relation to each weight. A **gradient** indicates how much the loss would change if we slightly adjusted the weight. If modifying a weight leads to a significant reduction in loss, the network recognizes that it should adjust that weight more substantially.

##### Step 3: Updating Weights with Optimization Algorithms
**Optimization algorithms** use these gradients to adjust the weights, aiming to reduce the error. Common algorithms include **Stochastic Gradient Descent (SGD)** and **Adam**. This is like tweaking your experimental conditions to get better results next time.

#### Iterative Learning Over Epochs
The network repeats this process over many cycles, called **epochs**. In each epoch, it uses the entire dataset once. With each pass, the network's predictions should improve, similar to refining your technique with practice.

#### Putting It All Together: An Analogy

Imagine you're learning to adjust a microscope to get the clearest image. You start by setting the focus and illumination based on a guess and then observe the image, which represents the network's prediction. Noticing that the image is blurry, you calculate the error using a loss function. Reflecting on which settings might be causing the blurriness, you adjust the focus and illumination accordingly, akin to backpropagation and optimization in neural networks. This process continues as you repeatedly refine your technique based on feedback, just like a neural network adjusts its weights based on the error feedback it receives, until the image is finally clear.
#### Why Non-Linearity Matters

Without activation functions introducing non-linearity, the network would only be able to model linear relationships—like a straight line on a graph. Biological systems are complex and often non-linear, so activation functions allow the network to capture these intricate patterns.


By iteratively adjusting and improving, neural networks learn to make accurate predictions from complex data—much like how you become more proficient in the lab through practice and learning from mistakes.

This process is all about the network learning from errors and adjusting itself to improve, similar to how the scientific method relies on hypothesis, experimentation, observation, and refinement.




### tl;dr
Deep learning is revolutionizing the analysis of complex biological data by automating feature extraction through neural networks, which mimic human learning. Unlike traditional machine learning that requires manual feature selection, deep learning models can recognize intricate patterns directly from raw data, making them highly effective for applications in genomics, proteomics, and drug discovery. The learning process involves forward propagation, where data flows through layers to make predictions, followed by backpropagation, which adjusts the network's connections to minimize errors and improve accuracy over time.

Deep learning has significantly transformed biochemistry, particularly in protein structure prediction and drug discovery, improving accuracy and accelerating the identification of potential therapies. For students in this field, mastering deep learning techniques is essential for enhancing research capabilities and fostering collaboration across disciplines, while also opening up various career opportunities.


## Attention is all you need. From Deep Learning to Protein Learning Models


Generative Artificial Intelligence (AI) marks a significant leap in how machines interact with data. Unlike traditional AI models that perform specific tasks based on predefined rules, generative AI models can create new content—such as text, images, or even protein sequences—by learning patterns from existing data. 

In this section, we'll explore how generative AI evolved from deep learning, focusing on the pivotal roles of **Reinforcement Learning (RL)** and the **Transformer model** introduced in the groundbreaking paper *"Attention Is All You Need."* 

### The Role of Reinforcement Learning in Generative AI


Traditional deep learning models excel at recognizing patterns and making predictions based on labeled data—a process known as supervised learning. However, they often lack the ability to make decisions or improve over time through feedback. This limitation is where **Reinforcement Learning (RL)** comes into play.

RL introduces a framework where models learn to make a sequence of decisions by interacting with an environment to achieve a specific goal. The model, or agent, takes actions and receives feedback in the form of rewards or penalties. This trial-and-error approach enables the agent to learn optimal strategies over time.


#### RL in simple terms

Imagine you're a biochemist working in the lab, trying to optimize a reaction to produce the maximum yield of a desired product. Each time you adjust the reaction conditions—like temperature, pH, or enzyme concentration—you observe the outcome. Some adjustments lead to better yields (rewards), while others result in poorer outcomes or even failed experiments (penalties). Over time, you learn which adjustments work best through a process of trial and error.

RL operates on a similar principle but with models (agents) instead of scientists. Here's how it works:

The **environment** encompasses everything the agent interacts with; in our analogy, it represents the laboratory where various reactions unfold. The **agent** serves as the model or AI striving to achieve a specific goal, much like a virtual biochemist. **Actions** refer to the decisions or modifications the agent implements, akin to altering reaction conditions in the lab. Finally, **rewards and penalties** are the feedback mechanisms the agent receives based on its actions, with high yields serving as rewards and failed reactions acting as penalties.




Think of RL as similar to **enzyme optimization**:

- **Enzyme Design**: Suppose you’re engineering an enzyme to catalyze a reaction more efficiently. You make mutations to the enzyme’s active site and test their effects.
- **Mutations (Actions)**: Each mutation is an action taken by you (the agent) to improve the enzyme.
- **Activity Assay (Feedback)**: You measure the enzyme’s activity after each mutation.
	- **Increased Activity (Reward)**: Indicates a successful mutation.
	- **Decreased Activity (Penalty)**: Indicates an unsuccessful mutation.
- **Learning Process**: Over time, you identify which mutations enhance enzyme activity, refining your design strategy through continuous feedback.

Similarly, in RL:

- The **agent** makes adjustments (mutations).
- It **tests** these adjustments in the environment (enzyme assays).
- It **receives feedback** (activity levels).
- It **learns** the best strategies to maximize rewards (enzyme efficiency).


**In generative AI**, RL allows models not only to generate content but also to optimize it according to specific objectives, such as coherence, relevance, or ethical guidelines. For instance, in conversational AI like ChatGPT, RL helps the model decide not just what to say but how to say it in a way that aligns with user expectations.



Volodymyr Mnih and his colleagues at DeepMind made significant strides in Reinforcement Learning (RL) with their groundbreaking Nature paper *Human-level control through deep reinforcement learning*. They introduced the Deep Q-Network (DQN), which effectively merges Q-learning with deep convolutional neural networks. This approach enables the model to learn directly from high-dimensional sensory inputs—like raw pixels from Atari 2600 games—leading to human-level performance on various benchmarks. Key innovations in their work include end-to-end learning, which eliminates the need for manual feature engineering, and mechanisms like experience replay and target networks to stabilize and enhance the learning process.

The impact of their research has been profound, spurring greater interest and investment in deep reinforcement learning. The principles outlined in their paper have served as a foundation for numerous subsequent models and algorithms. Beyond gaming, the techniques developed have found applications in diverse fields such as robotics, autonomous driving, and other complex decision-making scenarios. Mnih and his team's work represents a pivotal moment in the evolution of AI, demonstrating the power of combining deep learning with RL to tackle complex environments.



#### Reinforcement Learning from Human Feedback (RLHF)

Reinforcement Learning from Human Feedback (RLHF) is an advanced application in generative AI where a model is trained using evaluations from human reviewers who assess the quality of its outputs. Initially, the model undergoes unsupervised pretraining on large datasets to grasp language patterns, followed by supervised fine-tuning on specific tasks. Finally, feedback from human evaluators is utilized to refine the model's performance through reinforcement learning.
This iterative process enables the model to learn subtle qualities like conversational tone, relevance, and appropriateness—attributes that are difficult to capture through supervised learning alone.


### The Emergence of the Transformer Model


Before the introduction of the Transformer model, sequence processing in natural language relied heavily on architectures like **Recurrent Neural Networks (RNNs)** and **Long Short-Term Memory (LSTM)** networks. These models processed data sequentially, which made it challenging to capture long-range dependencies in data and limited their ability to handle large datasets due to computational inefficiency.

#### "Attention Is All You Need": The Transformer Breakthrough

In 2017, the paper *"Attention Is All You Need"* by Vaswani et al. introduced the **Transformer** model, revolutionizing the field of natural language processing and, by extension, generative AI.

The core innovation of the Transformer is the **self-attention mechanism**, which allows the model to weigh the importance of different parts of the input data relative to each other. This mechanism enables the model to capture relationships between words in a sentence or amino acids in a sequence, regardless of their position.

In protein folding, amino acids that are far apart in the primary sequence may interact closely in the three-dimensional structure due to folding. Similarly, the self-attention mechanism allows the Transformer to consider the entire sequence and understand contextual relationships.

##### Transformer Architecture: Encoders and Decoders

The Transformer consists of two main components: the encoder and the decoder. The encoder processes the input data and creates a comprehensive representation, while the decoder uses this representation to generate the output, such as translating a sentence or predicting the next word. Each component is made up of layers that utilize self-attention and feed-forward neural networks, enabling the model to capture complex patterns in the data.


The Transformer employs **multi-head attention**, which allows it to focus on different parts of the input simultaneously. This is akin to examining multiple aspects of a biochemical process at once—such as enzyme kinetics, substrate specificity, and regulatory mechanisms—to gain a holistic understanding.


### Integration of Reinforcement Learning and Transformers

Integrating the Transformer's deep context understanding with reinforcement learning's goal-oriented optimization has made generative AI models increasingly interactive and adaptable. As a result, these models not only produce coherent responses but also better align with user intent. Additionally, RLHF allows them to learn from human interactions, ensuring they adhere to ethical guidelines and avoid generating inappropriate content. This approach also enables fine-tuning for specific tasks or styles, whether it's technical writing or creative storytelling, providing a more customized experience.


**ChatGPT** is a prime example of integrating the Transformer architecture with RLHF. It undergoes a pretraining phase where the model learns language patterns from a vast corpus of internet text. Following this, it is fine-tuned using supervised learning on example conversations. The model further enhances its abilities through reinforcement learning, where human evaluators rate its responses and the model adjusts based on this feedback. This comprehensive process results in a model capable of engaging in complex, nuanced conversations, adapting to user input, and providing informative and contextually appropriate responses.

### The Shift from Supervised to Unsupervised Learning


Supervised learning relies on labeled datasets, which can be time-consuming and costly to produce, particularly in fields such as biochemistry where labeled data may often be scarce or incomplete. In contrast, unsupervised learning enables models to learn from unlabeled data by identifying patterns and structures within the data itself. This approach is akin to exploratory research in biochemistry, where scientists observe natural phenomena to formulate hypotheses without the reliance on prior labels. By utilizing unsupervised learning, models can uncover insights that are not immediately apparent, leading to deeper understanding and innovation.

A prime example of this methodology is seen in models like the Transformer, which can be pretrained on extensive amounts of data to grasp language structures. This pretraining serves as a foundational step, allowing for fine-tuning on specific tasks and enabling the generation of novel content even when extensive labeled datasets are unavailable. The ability of these models to learn without precise labels reflects a broader trend in machine learning, where leveraging large, unlabeled datasets can yield successful outcomes.

We can clarify these concepts by drawing parallels between generative AI and biochemistry. For instance, the self-attention mechanism in Transformers resembles protein folding, where interactions between amino acids, even those distant in the primary sequence, influence the final structure. Similarly, enzyme optimization through mutations mirrors reinforcement learning, where agents adjust actions based on feedback to enhance performance. Lastly, the exploratory nature of unsupervised learning aligns with how scientists delve into unknown biochemical pathways, paving the way for discoveries that may not have been achievable through traditional supervised methods.

### tl;dr

Generative Artificial Intelligence (AI) represents a significant advancement in machine interaction with data, enabling the creation of new content by learning from existing information. Unlike traditional AI that follows predefined rules, generative AI innovates across fields like natural language processing, art, and biochemistry.

A key component of this evolution is the Transformer model, which enhances the ability of generative AI to understand and generate human-like text. Combined with reinforcement learning, models can make decisions by interacting with their environment to achieve specific goals, learning from feedback in the form of rewards or penalties to optimize outcomes over time.

This process is akin to real-world tasks, such as enzyme optimization, where systematic adjustments lead to improved results. By employing both RL and Transformer architecture, generative AI can produce and refine content to meet specific criteria like coherence and relevance.

## Language of Life: AI and the Next Generation of Protein Design
### From Physics to Proteins: The Nobel Prizes and AI's Impact on Protein Study

The 2024 Nobel Prizes in Physics and Chemistry shine a spotlight on the transformative power of artificial intelligence in advancing scientific discovery, particularly within the realm of protein research. Traditionally, the study of proteins has relied heavily on experimental techniques such as X-ray crystallography and NMR spectroscopy. These methods, while invaluable, are often time-consuming and costly, limiting the pace at which new insights can be gained. However, the advent of AI has revolutionized this landscape, enabling researchers to study and design proteins using computational methods. This shift has unlocked new possibilities for understanding the fundamental building blocks of life, accelerating discoveries that were once thought to be out of reach.

In the field of Physics, the 2024 Nobel Prize was awarded to John Hopfield and Geoffrey Hinton for their pioneering work on artificial neural networks. Their research, which began in the 1980s, laid the essential groundwork for modern AI, particularly in the area of machine learning through artificial neural networks. Hopfield's 1982 model for associative memory, based on a recurrent neural network, drew intriguing parallels between artificial neural networks and spin models in statistical physics. This innovative connection was pivotal, paving the way for the development of sophisticated algorithms that can mimic the learning and processing capabilities of the human brain. Their contributions have been instrumental in shaping the AI technologies that underpin today's advanced computational tools.

Meanwhile, the 2024 Nobel Prize in Chemistry was awarded to David Baker, Demis Hassabis, and John Jumper for their significant contributions to computational protein design and protein structure prediction. The quest to predict a protein's three-dimensional structure from its amino acid sequence has been a longstanding challenge in biochemistry, spanning over half a century. This journey began with Christian Anfinsen’s Nobel Prize-winning discovery in 1972, which established that a protein's 3D structure is determined solely by its amino acid sequence. Determining protein structures has traditionally been a laborious process, requiring intricate experimental techniques. These efforts culminated in the creation of the Protein Data Bank, a comprehensive repository of experimentally determined protein structures that has been instrumental in numerous scientific breakthroughs.

David Baker's work showcases the important role of computational tools in protein research. He developed the Rosetta software, which allows scientists to design new proteins. Originally, Rosetta focused on creating stable protein structures, but its functions have since grown to include designing proteins for specific purposes.

Baker’s groundbreaking work in designing new enzymes illustrated how computational models can create enzymes that perform reactions not found in nature. Although these initial designs had lower catalytic rates than natural enzymes, techniques like directed evolution—recognized by Frances Arnold’s Nobel Prize in 2018—led to significant improvements. This highlighted the great potential of combining computer-based design with experimental testing, effectively connecting theoretical models to practical applications.

On the other hand, Demis Hassabis and John Jumper, leading the team at Google DeepMind, developed AlphaFold. This AI system uses deep learning to predict protein structures with unprecedented accuracy. AlphaFold's performance was spectacularly demonstrated in the Critical Assessment of Protein Structure Prediction (CASP) competition, where it consistently outperformed other methods. This breakthrough has led to databases containing millions of predicted protein structures, vastly expanding the resources available for protein research. AlphaFold has accelerated the pace of discovery and provided a robust framework for understanding protein function and interactions, which is essential for advancements in drug discovery and biotechnology.

The 2024 Nobel Prizes in Physics and Chemistry underscore the pivotal role of AI in transforming protein science. The foundational work on neural networks by Hopfield and Hinton has enabled sophisticated AI models like AlphaFold and protein language models, revolutionising our ability to predict and design proteins. These AI-driven tools have shifted protein research from predominantly experimental to increasingly computational, offering biochemists new ways to explore and manipulate the molecular machinery of life. 

### Demystifying Protein Language Models

Imagine being able to predict the intricate three-dimensional structure of a protein simply by "reading" its amino acid sequence, much like deciphering the meaning of a sentence by understanding the arrangement of its letters. This fascinating capability is at the heart of **Protein Language Models (PLMs)**. PLMs treat protein sequences as a language, where each amino acid acts as a letter, and the sequence of these amino acids forms sentences that dictate the protein's structure and function.

Proteins can be thought of as life sentences, with the 20 standard amino acids serving as the letters of the protein alphabet. Just as the arrangement of letters determines the meaning of a sentence, the sequence of amino acids dictates a protein’s three-dimensional structure and biological role. Within this analogy, protein domains—functionally distinct units within a protein—are akin to words or phrases, each contributing to the overall meaning conveyed by the protein's structure.

### How Protein Language Models Work

At the core of PLMs are sophisticated neural networks, often built using transformer architectures. These models excel at processing sequential data by employing an attention mechanism to focus on the most relevant parts of the input sequence. Training a PLM involves feeding it vast datasets of protein sequences, enabling the model to learn the complex relationships between amino acids and their structural and functional consequences. Imagine teaching a student to understand language by having them read millions of books; similarly, PLMs learn the "grammar" of proteins by analysing millions of protein sequences from diverse organisms.

A key concept in PLMs is **embeddings**, representing each amino acid as a unique point in a multi-dimensional space. This embedding space is meticulously organised so that amino acids with similar properties—such as size, charge, or hydrophobicity—are positioned close to each other. It’s like creating a map where cities with similar climates and cultures are clustered together. This spatial arrangement allows PLMs to capture the nuanced relationships between amino acids, helping the model understand how different sequences fold into functional proteins.

### Unveiling the Power of Protein Language Models

Once trained, PLMs become invaluable tools for studying and designing proteins. One of their most groundbreaking applications is **predicting protein structure**. Determining a protein’s three-dimensional arrangement from its amino acid sequence has been a long-standing challenge in biochemistry. PLMs like DeepMind’s **AlphaFold** have achieved remarkable accuracy, rivalling traditional experimental methods. By accurately predicting protein structures, PLMs accelerate our understanding of protein functions and interactions, which is crucial for drug discovery and biotechnology advancements.

PLMs also excel in **predicting protein function**. By analysing a protein sequence, these models can identify similarities to proteins with known functions, allowing researchers to infer the roles of newly discovered proteins. This capability is akin to recognising the function of a new enzyme by comparing its structure to that of enzymes whose functions are already understood.

Moreover, PLMs open up exciting possibilities in **designing novel proteins**. They can generate entirely new protein sequences that fold into desired structures, expanding the repertoire of protein architectures beyond those found in nature. This de novo protein design is comparable to creating new words or phrases in a language to express ideas that previously couldn’t be articulated. Additionally, PLMs facilitate **targeted protein engineering**, enabling the design of proteins with specific properties such as enhanced stability, catalytic activity, or binding affinity. Imagine engineering an enzyme to work more efficiently under industrial conditions or designing a protein that can bind to a specific drug molecule with high affinity.

### PLMs on protein science


#### Predicting Protein Properties

One of the most remarkable capabilities of PLMs is their proficiency in predicting various protein properties directly from amino acid sequences. This includes:

**Protein Structure:** PLMs have demonstrated impressive accuracy in predicting aspects of protein structure, from secondary elements like alpha-helices and beta-sheets to complex three-dimensional folds. They sometimes surpass traditional methods that rely heavily on multiple sequence alignments. By understanding the sequence patterns that lead to specific structural formations, PLMs provide valuable insights into how proteins attain functional conformations.

**Protein Function:** Beyond structure, PLMs can predict the functions of proteins, such as enzymatic activities or roles in cellular processes. This is particularly exciting for characterising unannotated proteins in biological databases. Analysing sequence similarities and patterns, PLMs help identify potential functions, guide experimental validation, and expand our understanding of biological mechanisms.

**Evolutionary Fitness:** PLMs delve into the evolutionary history of proteins by evaluating the fitness of different sequence variants. They assess how mutations might impact protein function, stability, or interactions, allowing researchers to trace evolutionary pathways and understand how proteins have adapted. This capability is crucial for studying disease-associated mutations and evolutionary biology.

**Post-Translational Modifications (PTMs):** PTMs like phosphorylation or glycosylation are critical for regulating protein activity and function. PLMs can predict potential modification sites within protein sequences, aiding in identifying regulatory mechanisms and facilitating the study of signalling pathways. This computational prediction is invaluable, given the challenges of experimentally identifying PTMs.

#### Understanding Protein Evolution

PLMs offer powerful tools for tracing evolutionary relationships between proteins. By analysing patterns of amino acid substitutions across diverse species and evolutionary timescales, they reveal how proteins have diversified and adapted to different environments and functions. This deep evolutionary insight helps reconstruct phylogenetic trees and understand the molecular basis of adaptation and speciation.

#### Analyzing Protein Interactions

Protein-protein interactions (PPIs) are fundamental to virtually all biological processes. PLMs enhance our ability to study PPIs by examining sequence features contributing to binding interfaces. They can predict how proteins interact, identify potential interaction partners, and even estimate binding affinities. This knowledge is crucial for understanding cellular networks and can inform the design of therapeutics that target or mimic specific protein interactions.




### Innovating Protein Design with Protein Language Models

As we venture further into the exciting intersection of biochemistry and artificial intelligence, **Protein Language Models (PLMs)** are emerging as transformative tools in protein design and engineering. Initially celebrated for their ability to decode the complexities of protein sequences, PLMs are now revolutionising how we create new proteins with desired structures and functions. By learning the "grammar" of amino acids, these models open up unprecedented possibilities in synthetic biology.

#### Direct Sequence Design: Expanding the Protein Universe

Traditionally, protein design has often involved modifying existing proteins or searching for sequences that fit a predetermined structural framework. PLMs are changing this paradigm by enabling **direct sequence design**. By training on vast datasets of protein sequences, PLMs learn the underlying patterns and relationships within the protein "language." This knowledge allows them to generate entirely new protein sequences without relying on predefined backbone structures.

Imagine having a vast dictionary of protein sequences and being able to craft new "sentences" that make sense within this language. This approach broadens the exploration of sequence space, increasing the likelihood of discovering proteins with novel and potentially valuable properties. For instance, PLMs have been used to generate new antibody sequences by learning from databases of known antibodies, leading to the development of antibodies with specific binding properties.

#### De Novo Protein Design: Crafting Proteins from Scratch

Creating proteins entirely from scratch, known as **de novo protein design**, has always been a formidable challenge, requiring a deep understanding of protein folding and function. PLMs are making significant strides in this area by generating new backbone structures and optimising amino acid sequences to enhance stability and functionality.

For example, researchers have utilised PLMs to design novel TIM barrel proteins—a common enzyme fold—creating versions with unique shapes suitable for incorporating small-molecule binding sites. This capability opens doors to designing enzymes that catalyse reactions not found in nature or creating proteins with entirely new functions.

#### Functional Motif Scaffolding: Building Proteins Around Key Features

Another exciting application of PLMs is **functional motif scaffolding**. Here, scientists define a desired functional motif, such as a catalytic site or binding domain, and use a PLM to generate a protein scaffold that supports and stabilises this motif. It's akin to designing a custom frame around a precious gem, ensuring it is held securely while enhancing its beauty.

This method allows for creating proteins with specific functions even when natural sequences that perform those functions are unknown. For instance, PLMs have been used to design proteins that effectively position catalytic residues, resulting in enzymes with enhanced activity.

### Integrating PLMs with Traditional Design Tools

While PLMs are powerful, their integration with established protein design tools amplifies their effectiveness. Combining the data-driven insights of PLMs with physics-based methods like **Rosetta** creates a robust pipeline for protein engineering. A PLM might generate an initial set of candidate sequences, which are then refined and evaluated using Rosetta to optimise their structural stability and minimise energy states.

This synergy between AI-driven models and traditional computational methods enhances protein design's efficiency and accuracy. It allows researchers to navigate the vast protein sequence space more effectively, homing in on innovative and viable designs.

#### Generative Design with Diffusion Models: A New Frontier

Pushing the boundaries further, PLMs are being integrated into **diffusion models** for protein design. In this approach, a protein structure is gradually "denoised" from a random starting point, guided by the PLM's learned knowledge. This method enables the generation of diverse protein structures and sequences predicted to be stable and functional without predefined templates.

An example is the **FoldingDiff** model, which can produce high-quality protein structures directly. This generative design approach offers a new frontier in protein engineering, allowing for creating proteins with unprecedented architectures and functions.

#### Real-World Applications: From Enzymes to Therapeutics

The practical impact of protein language models (PLMs) in protein design is already evident through various successful applications. Scientists have developed enzymes with improved catalytic activity and stability and immunoglobulin folds that are crucial for antibody development. Additionally, they have created protein-based pH sensors with targeted sensitivity ranges and membrane proteins that function as ion channels or receptors. 

These advancements are not merely theoretical; they hold significant potential for creating new therapeutics, industrial enzymes, and biomaterials. By leveraging the capabilities of PLMs, researchers can customise proteins to meet specific requirements, effectively addressing challenges in medicine, biotechnology, and environmental science.



### Overcoming Challenges and Paving the Way Forward

Despite their impressive capabilities, PLMs face several challenges. Incorporating more **biological knowledge** into these models is essential for improving their accuracy and relevance. While PLMs excel at identifying patterns in protein sequences, integrating information about protein stability, folding kinetics, and evolutionary constraints can lead to more biologically meaningful predictions and designs.

**Interpretability** is another critical area of focus. Understanding how PLMs make their predictions is crucial for ensuring that the models are reliable and that their outputs are based on sound biological principles. Developing methods to interpret the internal workings of PLMs will help researchers gain deeper insights into protein biology and validate the models’ predictions.

Additionally, the **quality and diversity of training data** play a significant role in the performance of PLMs. Biased or incomplete datasets can lead to inaccurate or misleading predictions. Ensuring that PLMs are trained on high-quality, representative datasets is vital for their success and reliability in protein research.


## A New Era for Biochemistry

As we stand at the threshold of unprecedented scientific advancement, the fusion of **artificial intelligence (AI)** and biochemistry heralds a new era—one rich with potential for innovation, discovery, and the betterment of human health. For aspiring biochemists, this convergence is not just an abstract concept but a tangible force reshaping our field's very foundations. Integrating AI into protein science, protein engineering, and computational biology accelerates our ability to decipher complex biological systems, design novel proteins, and develop cutting-edge therapeutics. This is an exciting time to be a part of the biochemistry community, where your curiosity and creativity can contribute to once-impossible breakthroughs.

### Embracing the AI Revolution in Biochemistry

Biochemistry has always been at the forefront of understanding life at the molecular level. However, biological data's sheer complexity and scale have often posed significant challenges. Traditional experimental techniques, while invaluable, are time-consuming and sometimes limited in scope. Enter **artificial intelligence**—a catalyst that amplifies our capacity to analyse, interpret, and manipulate biological data with unprecedented speed and precision.

**Protein Language Models (PLMs)** and deep learning algorithms are revolutionising how we study proteins. These AI-driven tools can predict protein structures with remarkable accuracy, design novel enzymes, and uncover relationships within vast datasets that would be impossible to detect manually. For instance, models like **AlphaFold** have transformed our approach to protein folding problems, providing insights that accelerate drug discovery and the development of new therapies.

### Unleashing Creativity in Protein Design and Engineering

The advent of AI in protein engineering is not merely about automation; it's about unleashing creativity. With tools like PLMs, you can venture into **de novo protein design**, crafting entirely new proteins with functions tailored to specific needs. Imagine designing an enzyme that catalyses a reaction not found in nature or creating a protein that can neutralise a toxin. These possibilities are no longer confined to science fiction but are accessible through the integration of AI and biochemistry.

**Functional motif scaffolding** and **direct sequence design** allow you to build proteins around desired features, opening doors to innovative solutions in medicine, environmental science, and industry. The ability to generate and test countless protein variants computationally accelerates the iterative design and experimentation process, leading to more effective and efficient outcomes.

### The Power of Interdisciplinary Collaboration

This new era is characterized by the blurring of boundaries between disciplines. Computational biology, data science, and biochemistry are converging, creating a landscape where interdisciplinary collaboration is not just beneficial but essential. By embracing computational tools and AI, you position yourself at the cutting edge of research, equipped to tackle complex challenges that require a multifaceted approach.

Engaging with AI also means contributing to its responsible development. Ethical considerations, such as data privacy, algorithmic bias, and the societal impacts of biotechnological advancements, are integral to the conversation. As future leaders in the field, your awareness and guidance in these areas will shape how technologies are applied for the greater good.

### An Invitation to Innovate

We are living in a time where your contributions can have a profound impact on the world. The tools and technologies at your disposal are more potent than ever, but they require your insight, creativity, and passion to realise their full potential. Whether it's developing new treatments for diseases, engineering sustainable solutions to environmental challenges, or uncovering the fundamental principles of life, the opportunities are vast and varied.

As you embark on your journey in biochemistry, consider diving into computational biology and AI. Learn to code, explore machine learning algorithms, and experiment with protein design software. Collaborate with peers in computer science and engineering. Stay curious and open to new ideas. The skills you acquire will enhance your research capabilities and make you a versatile scientist prepared to lead in an evolving scientific landscape.

### A Future Defined by Innovation and Discovery

The convergence of AI and biochemistry is more than a technological advancement; it's a paradigm shift that redefines what's possible in science. It's an invitation to push the boundaries of knowledge, to ask bold questions, and to seek answers that can transform our understanding of biology and improve lives.

Embrace this era of innovation with enthusiasm and determination. Your efforts today will shape the discoveries of tomorrow. Together, leveraging the synergy of AI and biochemistry, we can unlock the mysteries of life and pioneer solutions to some of humanity's most pressing challenges.

---

**Further Exploration:**

- **Get Involved in Research Projects:** Seek out laboratories and research groups that focus on computational biology, protein engineering, or AI applications in biochemistry.
- **Expand Your Skill Set:** Consider courses or workshops in machine learning, programming languages like Python, and bioinformatics tools.
- **Join Interdisciplinary Teams:** Collaborate with students and professionals in computer science, mathematics, and engineering to gain diverse perspectives.
- **Stay Informed:** Keep up with the latest advancements by reading scientific journals, attending seminars, and participating in conferences related to AI and biochemistry.

*The future of biochemistry is yours to shape. Dive in, explore, and let your curiosity lead the way.*




## Further reading

Here’s the list sorted alphabetically by the author's last name:

1. Amodei, D. *Machines of Loving Grace* (2024). https://darioamodei.com/machines-of-loving-grace
2. Angermueller, C., Pärnamaa, T., Parts, L. & Stegle, O. *Deep learning for computational biology.* Mol Syst Biol 12, 878 (2016).
3. Baek, M. & Baker, D. *Deep learning and protein structure modeling.* Nat Methods 19, 13–14 (2022).
4. Baker, R. E., Peña, J.-M., Jayamohan, J. & Jérusalem, A. *Mechanistic models versus machine learning, a fight worth fighting for the biological community?* Biol. Lett. 14, 20170660 (2018).
5. Baldi, P. *Deep Learning in Biomedical Data Science.* Annu Rev Biomed Data Sci 1, 181–205 (2018).
6. Bordin, N., Dallago, C., Heinzinger, M., Kim, S., Littmann, M., Rauer, C., et al. *Novel machine learning approaches revolutionize protein knowledge.* Trends Biochem Sci. 48, 345–59 (2023).
7. Chen, Z. & Elowitz, M. B. *Programmable protein circuit design.* Cell 184, 2284–2301 (2021).
8. Chicco, D. *Ten quick tips for machine learning in computational biology.* BioData Min. 10, 35 (2017).
9. Dauparas, J., Anishchenko, I., Bennett, N., Bai, H., Ragotte, R. J., Milles, L. F., et al. *Robust deep learning–based protein sequence design using ProteinMPNN.* Science. 378, 49–56 (2022).
10. Denning, P. *Computational Thinking in Science.* American Scientist. 105, 13 (2017).
11. Domingos, P. *A few helpful things to know about machine learning.* Commun. ACM 55, 78–87 (2012).
12. Domingos, P. *The master algorithm.* Penguin Books (2017).
13. Ferranti, D. et al. *The value of prior knowledge in machine learning of complex network systems.* Bioinformatics. 33, 3610 (2017).
14. Ferruz, N. et al. *From sequence to function through structure: Deep learning for protein design.* Comput Struct Biotechnol J. 21, 238–250 (2023).
15. Ferruz, N. et al. *ProtGPT2 is a deep unsupervised language model for protein design.* Nat Commun. 13, 4348 (2022).
16. Greener, J. G., Kandathil, S. M., Moffat, L. & Jones, D. T. *A guide to machine learning for biologists.* Nat Rev Mol Cell Biol. 23, 40–55 (2022).
17. Hey, T. et al. *The Fourth Paradigm: Data-Intensive Scientific Discovery.* Microsoft Research (2009).
18. Jones, W., Alasoo, K., Fishman, D. & Parts, L. *Computational biology: deep learning.* Emerg. Top. Life Sci. 1, 133–150 (2017).
19. Jumper, J. et al. *Highly accurate protein structure prediction with AlphaFold.* Nature. 596, 583 (2021).
20. LeCun, Y., Bengio, Y. & Hinton, G. *Deep learning.* Nature 521, 436–444 (2015).
21. Markowetz, F. *All biology is computational biology.* PLOS Biol. 15, e2002050 (2017).
22. Mnih, V. et al. *Human-level control through deep reinforcement learning.* Nature 518, 529–533 (2015).
23. Mollick, E. *Co-intelligence: living and working with AI.* Penguin Books (2024).
24. Newport, C. What Kind of Mind does ChatGPT Have? New Yorker (2023).  https://www.newyorker.com/science/annals-of-artificial-intelligence/what-kind-of-mind-does-chatgpt-have
25. Ovchinnikov, S. & Huang, P.-S. *Structure-based protein design with deep learning.* Curr Opin Chem Biol. 65, 136–144 (2021).
26. Rampasek, L. & Goldenberg, A. *TensorFlow: Biology’s Gateway to Deep Learning?* 2, 12–14 (2016).
27. Stevens, H. *Life Out of Sequence.* University of Chicago Press Kindle edition (2013).
28. Tegmark, M. *Life 3.0.* Penguin Books (2018).
29. Vaswani, A. et al. *Attention is all you need.* arXiv:1706.03762 (2017).
30. Webb, S. *Deep learning for biology.* Nature 554, 555–557 (2018).


  
  
  
  
  


